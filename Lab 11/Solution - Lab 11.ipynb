{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d638113-46b7-4e80-8749-8ce8339b32d0",
   "metadata": {},
   "source": [
    "<h1 align='center'>Machine Learning Lab</h1>\n",
    "<h3 align='center'>Lab 11</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00813c39-3e86-4761-9aee-9c84aacad130",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ed9922-2e2d-4036-8edf-ab4b7d7f3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                                #Importing Regular Expression\n",
    "import string                                            #Importing String\n",
    "import numpy as np                                       #Importing Numpy\n",
    "import pandas as pd                                      #Importing Pandas\n",
    "import random                                            #Importing Random\n",
    "from sklearn.datasets import fetch_20newsgroups          #Importing News Dataset\n",
    "from sklearn.svm import SVC                              #Importing SVM\n",
    "from sklearn.model_selection import GridSearchCV         #Importing Grid Search\n",
    "from sklearn.metrics import accuracy_score               #Importing Accuracy Score Metric\n",
    "from nltk.corpus import stopwords                        #Importing Stopwords\n",
    "from nltk.tokenize import word_tokenize                  #Importing NLTK Word Tokenizer\n",
    "import itertools                                         #Importing Iterator\n",
    "import warnings                                          #Importing Warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd22e7-0fff-4985-b78a-ea8a69e02589",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 0: Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1bc0a0-7bb1-4d08-81b5-58520d60668a",
   "metadata": {},
   "source": [
    "#### Reading the Dataset and taking Subset with two categories named as sci.med and comp.graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fc4eb1-34f1-4bd2-a930-dc751d65849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the value for Random seed\n",
    "random_seed = 3116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90ea04e-a52b-4b57-a240-81f3dfa2fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='all', categories=['sci.med','comp.graphics'], shuffle=True, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebc629-edab-41e5-b850-541dcb26a974",
   "metadata": {},
   "source": [
    "#### Preprocessing textual data to remove punctuation, stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583ff7d-6f63-45ad-be99-c30872656051",
   "metadata": {},
   "source": [
    "##### Function to Preprocess the data by removing Stopwords, punctuations and tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c176a73-be1b-4007-9ad9-f8165d45bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(news_string):\n",
    "    #Extracting the English stopwords and converting it into a set\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #Making the data into the lower case string and then tokenizing the data into word list\n",
    "    news_string = word_tokenize(news_string.lower())\n",
    "    \n",
    "    #Removing stopwords and punctuations from the word list\n",
    "    news_string = [word for word in news_string if word not in english_stop_words and word.isalpha()]\n",
    "    \n",
    "    #Returning the final processed data list\n",
    "    return news_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed7dfa-0d90-4439-9350-4825c1dfd0ee",
   "metadata": {},
   "source": [
    "##### Extracting News and its Target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba20fe3-51b8-430c-b6c3-97d9529e9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_items = news['data']\n",
    "news_target = news['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8534b2-eb0c-412e-82be-9588b7bcfc34",
   "metadata": {},
   "source": [
    "##### Applying Preprocessing step on all the News items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744c78bb-223a-430a-a20e-aff763722e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing an empty list to store processed news items\n",
    "processed_news = []\n",
    "\n",
    "#Iterating for each news items\n",
    "for n_item in news_items:\n",
    "    \n",
    "    #Applying preprocessing on current news item\n",
    "    processed_news.append(preprocess_data(n_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea64f9-af41-4d33-8e30-aef6abde1a2e",
   "metadata": {},
   "source": [
    "#### Implementing a bag-of-words feature representation for each text sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2c94c-cb55-4215-b6d2-bca0e00c6a04",
   "metadata": {},
   "source": [
    "##### Function to create a Word Frequency Dictionary from the Provided Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b211a5-db75-457e-837a-f4651a25685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_word_freq(data, freq_dict):\n",
    "    #Iterating for all words in the document\n",
    "    for word in data:\n",
    "        #If word is already present in the dictionary than we add 1\n",
    "        if word in freq_dict:\n",
    "            freq_dict[word] += 1\n",
    "        #If word is not present, then we create a new key and assign 1 as a value\n",
    "        else:\n",
    "            freq_dict[word] = 1\n",
    "    \n",
    "    #Returning the created word frequency dictionary\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f5b74-556e-4030-846b-2eceb91bbe56",
   "metadata": {},
   "source": [
    "##### Function to create a Binary vector for a document based on Bag of Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5590971-ba18-4587-bf3f-b9d37c4c37b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(data, freq_dict):\n",
    "    #Initializing a vector with zeros having the length equal to total unique words in the corpus\n",
    "    sentence_vector = np.zeros(shape=(len(freq_dict.keys()),))\n",
    "    \n",
    "    #Iterating over all words in the document\n",
    "    for word in data:\n",
    "        \n",
    "        #Placing 1 in the vector based on index assigned to that word in the dictionary\n",
    "        if word in freq_dict.keys():\n",
    "            sentence_vector[list(freq_dict.keys()).index(word)] = 1\n",
    "    \n",
    "    #Returning the document vector\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbff4b-e6f2-4326-9a15-1c48a6e29be3",
   "metadata": {},
   "source": [
    "##### Converting each document in the dataset into a Bag of Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd08ef1c-fd60-4c75-bb99-44c9c9993c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Features to consider for Processing\n",
    "features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b06d8660-2dd6-41d6-8b31-231813e3ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store all unique words and there count in the entire corpus\n",
    "corpus_word_freq = {}\n",
    "\n",
    "#Iterating for each document in the dataset\n",
    "for doc in processed_news:\n",
    "    \n",
    "    #Updating the dictionary for each document\n",
    "    corpus_word_freq = update_word_freq(doc, corpus_word_freq)\n",
    "\n",
    "#Sorting the dictionary in Descending Order\n",
    "corpus_word_freq = dict(sorted(corpus_word_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#Extracting only required words based on feature value\n",
    "corpus_word_freq = dict(itertools.islice(corpus_word_freq.items(), features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0593fefe-769c-4f04-9e76-09f8962e9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a empty list to store the Bag of Word representation for each document\n",
    "news_bog_vectors = []\n",
    "\n",
    "#Iterating for each document in the dataset\n",
    "for doc in processed_news:\n",
    "    \n",
    "    #Creating a bag of word representation vector and appending it into the final list\n",
    "    news_bog_vectors.append(bag_of_words(doc, corpus_word_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c72c1-d902-4ad7-b525-46bc854c958d",
   "metadata": {},
   "source": [
    "##### Displaying the created Bag of Word Representation in the form of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62cf8c35-b296-4eaa-994b-0727dd7ad6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>lines</th>\n",
       "      <th>organization</th>\n",
       "      <th>one</th>\n",
       "      <th>would</th>\n",
       "      <th>image</th>\n",
       "      <th>university</th>\n",
       "      <th>writes</th>\n",
       "      <th>article</th>\n",
       "      <th>also</th>\n",
       "      <th>...</th>\n",
       "      <th>media</th>\n",
       "      <th>dan</th>\n",
       "      <th>exercise</th>\n",
       "      <th>request</th>\n",
       "      <th>den</th>\n",
       "      <th>goes</th>\n",
       "      <th>terms</th>\n",
       "      <th>shown</th>\n",
       "      <th>centers</th>\n",
       "      <th>dangerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  lines  organization  one  would  image  university  writes  \\\n",
       "0      1.0    1.0           1.0  0.0    0.0    0.0         0.0     0.0   \n",
       "1      1.0    1.0           1.0  0.0    0.0    0.0         0.0     0.0   \n",
       "2      1.0    1.0           1.0  1.0    0.0    0.0         0.0     0.0   \n",
       "3      1.0    1.0           1.0  0.0    1.0    0.0         0.0     0.0   \n",
       "4      1.0    1.0           1.0  1.0    0.0    0.0         0.0     0.0   \n",
       "\n",
       "   article  also  ...  media  dan  exercise  request  den  goes  terms  shown  \\\n",
       "0      0.0   0.0  ...    0.0  0.0       0.0      0.0  0.0   0.0    0.0    0.0   \n",
       "1      0.0   0.0  ...    0.0  0.0       0.0      0.0  0.0   0.0    0.0    0.0   \n",
       "2      0.0   0.0  ...    0.0  0.0       0.0      0.0  0.0   1.0    0.0    0.0   \n",
       "3      0.0   0.0  ...    0.0  0.0       0.0      0.0  0.0   0.0    0.0    0.0   \n",
       "4      0.0   1.0  ...    0.0  0.0       0.0      0.0  0.0   0.0    0.0    0.0   \n",
       "\n",
       "   centers  dangerous  \n",
       "0      0.0        0.0  \n",
       "1      0.0        0.0  \n",
       "2      0.0        0.0  \n",
       "3      0.0        0.0  \n",
       "4      0.0        0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_bog_df = pd.DataFrame(news_bog_vectors, columns=corpus_word_freq.keys())\n",
    "news_bog_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f66d1a-17e3-4cfe-ae2e-143bde2598ab",
   "metadata": {},
   "source": [
    "#### Implementing a TF-IDF feature representation for each text sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7078406-90e1-4ff0-8dfb-13c00b6ad755",
   "metadata": {},
   "source": [
    "##### Function to calculate the Term Frequency (TF) for a word in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a02941-a542-48fb-a5a9-3f742e85ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(document, word):\n",
    "    #tf = (total frequency of word in a document) / (total words in a document)\n",
    "    return document[word]/sum(document.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e4ff6-6f6e-43b0-b647-977020e125b9",
   "metadata": {},
   "source": [
    "##### Function to calculate the Inverse Document Frequency (IDF) for a word in the entire Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "878f46d1-f46e-40d7-9be5-fc100e09d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(total_doc_freq, word, total_documents):\n",
    "    #idf = log(total documents / total frequency of word in all documents)\n",
    "    return np.log(total_documents/total_doc_freq[word] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5880e-eb7a-46ff-b28c-519c90e4de06",
   "metadata": {},
   "source": [
    "##### Function to calculate the TF-IDF of a all the words individually in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01197f67-0520-4c34-91d0-134189f0548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(document, total_doc_freq, total_documents):\n",
    "    #Initializing a vector of zeros with a lenght of total unique words in the entire corpus\n",
    "    document_vector = np.zeros(shape=(len(total_doc_freq.keys()),))\n",
    "    \n",
    "    #Iterating for each word in the document\n",
    "    for word in document.keys():\n",
    "        \n",
    "        #Checking if word exist in our feature set\n",
    "        if word in total_doc_freq.keys():\n",
    "            \n",
    "            # tf-idf = tf * idf\n",
    "            tf_idf = term_frequency(document, word) * inverse_document_frequency(total_doc_freq, word, total_documents)\n",
    "\n",
    "            #Inserting the calculated tf-idf for that word in the vector\n",
    "            document_vector[list(total_doc_freq.keys()).index(word)] = tf_idf\n",
    "        \n",
    "    #Returning the final vector containing tf-idf values\n",
    "    return document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72869cb-9468-4925-9223-814961133a5e",
   "metadata": {},
   "source": [
    "##### Converting each document in the dataset into a TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25185765-4714-4902-aa19-8720d937e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a vector to store tf-idf vectors for each document\n",
    "news_tfidf_vectors = []\n",
    "\n",
    "#Iterating for each documents\n",
    "for doc in processed_news:\n",
    "    #Creating a word frequency dictionary for current document\n",
    "    current_doc_dict = update_word_freq(doc, {})\n",
    "    \n",
    "    #Calculating and Appending the tf-idf vector in the final vector\n",
    "    news_tfidf_vectors.append(tf_idf(current_doc_dict, corpus_word_freq, len(processed_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d3492-d1d1-4dec-ac13-dc6bafbd16bd",
   "metadata": {},
   "source": [
    "##### Displaying the created TF-IDF Representation in the form of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6c09c1-830e-4d7e-885b-c6f67381ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>lines</th>\n",
       "      <th>organization</th>\n",
       "      <th>one</th>\n",
       "      <th>would</th>\n",
       "      <th>image</th>\n",
       "      <th>university</th>\n",
       "      <th>writes</th>\n",
       "      <th>article</th>\n",
       "      <th>also</th>\n",
       "      <th>...</th>\n",
       "      <th>media</th>\n",
       "      <th>dan</th>\n",
       "      <th>exercise</th>\n",
       "      <th>request</th>\n",
       "      <th>den</th>\n",
       "      <th>goes</th>\n",
       "      <th>terms</th>\n",
       "      <th>shown</th>\n",
       "      <th>centers</th>\n",
       "      <th>dangerous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013108</td>\n",
       "      <td>0.013391</td>\n",
       "      <td>0.013782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011704</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>0.012305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>0.011881</td>\n",
       "      <td>0.030896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008739</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>0.009188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007364</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.010067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject     lines  organization       one     would  image  university  \\\n",
       "0  0.013108  0.013391      0.013782  0.000000  0.000000    0.0         0.0   \n",
       "1  0.011704  0.011956      0.012305  0.000000  0.000000    0.0         0.0   \n",
       "2  0.011300  0.011544      0.011881  0.030896  0.000000    0.0         0.0   \n",
       "3  0.008739  0.008927      0.009188  0.000000  0.012308    0.0         0.0   \n",
       "4  0.007364  0.007523      0.007743  0.010067  0.000000    0.0         0.0   \n",
       "\n",
       "   writes  article      also  ...  media  dan  exercise  request  den  \\\n",
       "0     0.0      0.0  0.000000  ...    0.0  0.0       0.0      0.0  0.0   \n",
       "1     0.0      0.0  0.000000  ...    0.0  0.0       0.0      0.0  0.0   \n",
       "2     0.0      0.0  0.000000  ...    0.0  0.0       0.0      0.0  0.0   \n",
       "3     0.0      0.0  0.000000  ...    0.0  0.0       0.0      0.0  0.0   \n",
       "4     0.0      0.0  0.011768  ...    0.0  0.0       0.0      0.0  0.0   \n",
       "\n",
       "       goes  terms  shown  centers  dangerous  \n",
       "0  0.000000    0.0    0.0      0.0        0.0  \n",
       "1  0.000000    0.0    0.0      0.0        0.0  \n",
       "2  0.062734    0.0    0.0      0.0        0.0  \n",
       "3  0.000000    0.0    0.0      0.0        0.0  \n",
       "4  0.000000    0.0    0.0      0.0        0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_tfidf_df = pd.DataFrame(news_tfidf_vectors, columns=corpus_word_freq.keys())\n",
    "news_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe4dde-19b6-46de-ba6e-fcd43674a59a",
   "metadata": {},
   "source": [
    "#### Splitting the dataset randomly into train/validation/test splits according to ratios 80%:10%:10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48a4cc7f-65cd-4ac3-b8a8-87ebb2b319a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(news_vectors, targets, train_ratio, validation_ratio):\n",
    "    \n",
    "    #Combining the Feature columns and the target column into a single list\n",
    "    combined = list(zip(news_vectors, targets))\n",
    "    \n",
    "    #Randomly shuffle the rows in the list\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    #Calculating the training rows and validation rows\n",
    "    train_rows = int(len(combined) * train_ratio)\n",
    "    validation_rows = int(len(combined) * validation_ratio)\n",
    "\n",
    "    #Extracting X matrix and Y matrix from the combined list\n",
    "    X , y = list(zip(*combined))\n",
    "    X, y = list(X), list(y)\n",
    "\n",
    "    #Splitting X and y matrices into Training, Validation and Test set\n",
    "    X_train, X_val, X_test = X[:train_rows], X[train_rows:train_rows+validation_rows], X[train_rows+validation_rows:]\n",
    "    y_train, y_val, y_test = y[:train_rows], y[train_rows:train_rows+validation_rows], y[train_rows+validation_rows:]\n",
    "    \n",
    "    #Returning all the subsets\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630baf3b-e39e-4009-89cd-354ebdde74a7",
   "metadata": {},
   "source": [
    "### Exercise 1: Implementing Naive Bayes Classifier for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdb051-cdaa-4467-ae70-b9e559804ad8",
   "metadata": {},
   "source": [
    "#### Class to Represent the Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dedba9e-39ee-47b7-8239-3cc9d4f0ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_bayes:\n",
    "    #Contructor function\n",
    "    def __init__(self,feature_representation, dataset, target):\n",
    "        \n",
    "        #Checking if the feature representation value is valid otherwise Raising Exception\n",
    "        if feature_representation not in ['bog', 'tfidf']:\n",
    "            raise Exception('Invalid value provided for Feature Representation')\n",
    "        \n",
    "        #Feature Representation type - Bag of Words (bog) or TF-IDF (tfidf)\n",
    "        self.feature_representation = feature_representation\n",
    "        \n",
    "        #Feature Columns of the Dataset\n",
    "        self.dataset = np.array(dataset)\n",
    "        \n",
    "        #Target/Label Column of the Dataset\n",
    "        self.target = np.array(target)\n",
    "        \n",
    "        #Unique Target/Label values\n",
    "        self.unique_target = list(set(target))\n",
    "        \n",
    "        #Current Accuarcy of the Model\n",
    "        self.accuracy = 0\n",
    "        \n",
    "        #Combined Dataset containing both, the Feature Columns and the Target Column\n",
    "        self.combined = np.concatenate((self.dataset, self.target.reshape(-1,1)), axis = 1)\n",
    "    \n",
    "    #Function to do Predictions on the dataset provided and calculate the Accuracy\n",
    "    def predict(self):\n",
    "        \n",
    "        #Iterating over all the different documents in the dataset\n",
    "        for index, doc in enumerate(self.combined):\n",
    "            \n",
    "            #Initializing an empty list to store the predicted probability for each target value\n",
    "            tar_prob = []\n",
    "            \n",
    "            #Iterating over all unique target values for calculating there probabilities\n",
    "            for tar in self.unique_target:\n",
    "                \n",
    "                #Based on Feature representation type, calculating the probabiltiy\n",
    "                if self.feature_representation == 'bog':\n",
    "                    tar_prob.append(self.__calculate_prob_bog(doc, tar))\n",
    "                elif self.feature_representation == 'tfidf':\n",
    "                    tar_prob.append(self.__calculate_prob_tfidf(doc, tar))\n",
    "            \n",
    "            #Normalizing each probability so that it sums to 1\n",
    "            tar_prob = list(map(lambda x : x / (sum(tar_prob) + 1),tar_prob))\n",
    "            \n",
    "            #Extracting the class with the maximum Probability\n",
    "            predicted_class = self.unique_target[tar_prob.index(max(tar_prob))]\n",
    "            \n",
    "            #Checking if the predicted class is equal to the actual class\n",
    "            if predicted_class == self.target[index]:\n",
    "                self.accuracy += 1\n",
    "        \n",
    "        #Calculating its final accuracy\n",
    "        self.accuracy /= len(self.combined)\n",
    "    \n",
    "    #A private function to calculate the Probability for a document represented using Bag of Words\n",
    "    def __calculate_prob_bog(self, document, target):\n",
    "        \n",
    "        #Calculating the probability for a class itself\n",
    "        # P(target) = (Number of times that class appears in the dataset) / (total documents)\n",
    "        prob_class = len(self.target[np.where(self.target == target)]) / len(self.target)\n",
    "        \n",
    "        #Initializing the prior probability for each Word in the document\n",
    "        words_prior_prob = 1\n",
    "        \n",
    "        #Iterating over each word in the document\n",
    "        for i in range(len(document) - 1):\n",
    "            \n",
    "            #Only calculating the probability if the word exist in the document\n",
    "            if document[i] == 1:\n",
    "                \n",
    "                #Calculating the Prior probability of the word given the class\n",
    "                # P(w1 | target) = (Number of times the word occurs in all the document given the class) / (Number of time word occurs in all documents)\n",
    "                p_word_num = len(self.combined[np.where((self.combined[:,i] == 1) & (self.combined[:,-1] == target))])\n",
    "                p_word_den = len(self.combined[np.where(self.combined[:,-1] == target)])\n",
    "                \n",
    "                #Multiplying the current word prior probability with other words\n",
    "                words_prior_prob *= (p_word_num / p_word_den)\n",
    "                \n",
    "        #Returning the final probability -> P(class) * P(W | class)\n",
    "        return words_prior_prob * prob_class\n",
    "    \n",
    "    #A private function to calculate the Probability for a document represented using TF-IDF\n",
    "    def __calculate_prob_tfidf(self, document, target):\n",
    "        \n",
    "        #Calculating the probability for a class itself\n",
    "        # P(target) = (Number of times that class appears in the dataset) / (total documents)\n",
    "        prob_class = len(self.target[np.where(self.target == target)]) / len(self.target)\n",
    "        \n",
    "        #Initializing the prior probability for each Word in the document\n",
    "        words_prior_prob = 1\n",
    "        \n",
    "        #Iterating over each word in the document\n",
    "        for i in range(len(document) - 1):\n",
    "            \n",
    "            #Only calculating the probability if the word exist in the document\n",
    "            if document[i] == 1:\n",
    "                \n",
    "                #Calculating the Prior probability of the word given the class\n",
    "                # P(w1 | target) = (Number of times the word occurs in all the document given the class) / (Number of time word occurs in all documents)\n",
    "                p_word_num = sum(self.combined[np.where((self.combined[:,i] == 1) & (self.combined[:,-1] == target))])\n",
    "                p_word_den = sum(self.combined[np.where(self.combined[:,-1] == target)])\n",
    "                \n",
    "                #Multiplying the current word prior probability with other words\n",
    "                words_prior_prob *= (p_word_num / p_word_den)\n",
    "                \n",
    "        #Returning the final probability -> P(class) * P(W | class)\n",
    "        return words_prior_prob * prob_class\n",
    "    \n",
    "    #Function to display the Accuracy of the Model\n",
    "    def score(self):\n",
    "        feature_rep = 'Bag of Words' if self.feature_representation == 'bog' else 'TF-IDF'\n",
    "        return 'The Accuracy for Naive Bayes using {} Representation is {:.2f}%'.format(feature_rep, self.accuracy * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cc430-67dc-402c-bdc3-ffc61dc27d88",
   "metadata": {},
   "source": [
    "#### Using Bag of Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5ddb7-6e52-40fd-9d4e-4815f0e01171",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with Bag of Word Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "421efc99-39b5-4b68-a9af-5e2d094e93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(news_bog_vectors, news_target, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba257f97-5945-4477-8bb3-f7d7297ef311",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the Naive Bayes model on the training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad32d8d1-c45c-49f8-a509-5757e541d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train = Naive_bayes('bog', X_train, y_train)\n",
    "nb_train.predict()\n",
    "\n",
    "nb_validation = Naive_bayes('bog', X_val, y_val)\n",
    "nb_validation.predict()\n",
    "\n",
    "nb_test = Naive_bayes('bog', X_test, y_test)\n",
    "nb_test.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1bc7a9-b208-4f63-9d3b-53e4050e03b7",
   "metadata": {},
   "source": [
    "##### Calculating and Displaying the Training, Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb529977-463a-4b8f-904d-f4013e662707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: \n",
      "The Accuracy for Naive Bayes using Bag of Words Representation is 95.73%\n",
      "\n",
      "Validation Accuracy: \n",
      "The Accuracy for Naive Bayes using Bag of Words Representation is 97.45%\n",
      "\n",
      "Test Accuracy: \n",
      "The Accuracy for Naive Bayes using Bag of Words Representation is 98.98%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: \\n{}'.format(nb_train.score()))\n",
    "print('\\nValidation Accuracy: \\n{}'.format(nb_validation.score()))\n",
    "print('\\nTest Accuracy: \\n{}'.format(nb_test.score()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc235a-2b19-40bf-bc51-9f0aa079d8af",
   "metadata": {},
   "source": [
    "#### Using TF-IDF Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b39a7-2f23-4337-a8b9-f314aba30fce",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with TF-IDF Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "628ac9e0-fb52-4227-b2b3-1230f990da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(news_tfidf_vectors, news_target, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c791a56-3ffd-47cc-856d-b3ce598e23ff",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the Naive Bayes model on the training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d5586e0-9ea4-4c30-b7bd-52a933115244",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train = Naive_bayes('tfidf', X_train, y_train)\n",
    "nb_train.predict()\n",
    "\n",
    "nb_validation = Naive_bayes('tfidf', X_val, y_val)\n",
    "nb_validation.predict()\n",
    "\n",
    "nb_test = Naive_bayes('tfidf', X_test, y_test)\n",
    "nb_test.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d8fc0-b20c-4653-9e2e-51ff0d77fded",
   "metadata": {},
   "source": [
    "##### Calculating and Displaying the Training, Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a65eff73-0d03-48a4-bf0e-706126d7ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: \n",
      "The Accuracy for Naive Bayes using TF-IDF Representation is 50.06%\n",
      "\n",
      "Validation Accuracy: \n",
      "The Accuracy for Naive Bayes using TF-IDF Representation is 51.53%\n",
      "\n",
      "Test Accuracy: \n",
      "The Accuracy for Naive Bayes using TF-IDF Representation is 52.28%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: \\n{}'.format(nb_train.score()))\n",
    "print('\\nValidation Accuracy: \\n{}'.format(nb_validation.score()))\n",
    "print('\\nTest Accuracy: \\n{}'.format(nb_test.score()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8353d0d-76e6-4ecf-b786-fa003a25bd7c",
   "metadata": {},
   "source": [
    "The Accuracy of TF-IDF is low because we have only taken 1000 features.If we increase the number of features, the accuracy can improve subsequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f0543-cf58-4d8a-9821-f0f544507841",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2: Implementing SVM Classifier via Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff4a33-66df-4ae6-81b5-c4978981ebb9",
   "metadata": {},
   "source": [
    "#### Defining Hyperparameter Grid for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a352914-e0fb-49c8-bae1-2da6767c60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {'C' : [0.01,0.02,0.03],\n",
    "                       'kernel': ['linear', 'rbf'],\n",
    "                       'gamma': ['scale','auto']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f6953-a0ec-42e2-b485-7782cc446025",
   "metadata": {},
   "source": [
    "#### Using Bag of Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3ff36-7bec-455a-b387-5ef2db1533a1",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with Bag of Word Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c74658f-2271-465f-8749-3902e1ade769",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(news_bog_vectors, news_target, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb1b6f-9436-4009-ab7b-316dd2630e43",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the SVM model on the training set using Grid Search and different Hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa4c97f5-8c0f-4c02-b632-e06ab565ea87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=3116), n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.02, 0.03], 'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing a SVM model with the random seed\n",
    "svm = SVC(random_state=random_seed)\n",
    "\n",
    "#Creating a Grid Seach object with the SVM model and K-fold Cross validation\n",
    "grid_model = GridSearchCV(svm, hyperparameter_grid, n_jobs=-1, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "#Fitting the training dataset on SVM with different Hyperparameters\n",
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "726fb72d-2d86-4974-9d4d-5133a3d89414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter combination found for Bag of Word Representation after applying Grid Search: \n",
      "{'C': 0.03, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Hyperparameter combination found for Bag of Word Representation after applying Grid Search: \\n{}'.format(grid_model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59caeeb8-6a03-4a19-9825-7dab97de1fbb",
   "metadata": {},
   "source": [
    "##### Calculating and Displaying the Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8fb4990-2e62-47da-92d1-d6c43ce2371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on best Hyperparameters: 95.41\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_val, grid_model.predict(X_val)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a838d65-aba6-435e-b83a-e665c5459bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on best Hyperparameters: 96.45\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_test, grid_model.predict(X_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fafea4-9c8f-4750-bdad-dc835f430f39",
   "metadata": {},
   "source": [
    "#### Using TF-IDF Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ad22f-87cd-44fc-b2fc-7bf40de2da68",
   "metadata": {},
   "source": [
    "##### Splitting the Dataset with TF-IDF Representation into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4cb7e43-bed1-474a-80ca-7b2ac47a5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(news_tfidf_vectors, news_target, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b42a9e-84ee-4fa8-8877-af00d72ca664",
   "metadata": {},
   "source": [
    "##### Creating and Fitting the SVM model on the training set using Grid Search and different Hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d47f92cf-2bfa-400d-933d-843f9ed18b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=3116), n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.02, 0.03], 'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'rbf']},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing a SVM model with the random seed\n",
    "svm = SVC(random_state=random_seed)\n",
    "\n",
    "#Creating a Grid Seach object with the SVM model and K-fold Cross validation\n",
    "grid_model = GridSearchCV(svm, hyperparameter_grid, n_jobs=-1, cv=5, return_train_score=True)\n",
    "\n",
    "#Fitting the training dataset on SVM with different Hyperparameters\n",
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeabdce4-44a4-481f-a98d-f020fa103ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter combination found for TF-IDF Representation after applying Grid Search: \n",
      "{'C': 0.03, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Hyperparameter combination found for TF-IDF Representation after applying Grid Search: \\n{}'.format(grid_model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42131d-fb02-451a-9a6b-deac8372faa4",
   "metadata": {},
   "source": [
    "##### Calculating and Displaying the Validation and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dbefa63-0dca-4e94-823d-6c739c0bc58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy on best Hyperparameters: 74.49\n"
     ]
    }
   ],
   "source": [
    "print('Validation Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_val, grid_model.predict(X_val)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bbdbc09-eb4e-4a58-b73c-74e1ee3f212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on best Hyperparameters: 68.02\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy on best Hyperparameters: {:.2f}'.format(accuracy_score(y_test, grid_model.predict(X_test)) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
